time <attribute 'day' of 'datetime.date' objects> <attribute 'hour' of 'datetime.datetime' objects>:0001-01-01 00:00:00main file: DDPG_main.pyplatform = Windows
episodes = 250
buffer size = 1000000
gamma = 0.99
tau = 0.001
learning rate actor = 0.001
learning rate critic = 0.001
device = cuda:0
learn every = 20
learn repeat x times = 10
(noise) sigma = 0.2
pre-fill the buffer with experiences from random actions
Score ([min, mean, max] over agents) for ep. 0: [0.00,0.01,0.1] 	T: 0:21(m:s)	Est remain: 1:29(h:m)
Score ([min, mean, max] over agents) for ep. 1: [0.00,0.03,0.1] 	T: 0:20(m:s)	Est remain: 1:24(h:m)
Score ([min, mean, max] over agents) for ep. 2: [0.00,0.00,0.0] 	T: 0:21(m:s)	Est remain: 1:27(h:m)
Score ([min, mean, max] over agents) for ep. 3: [0.00,0.01,0.1] 	T: 0:22(m:s)	Est remain: 1:32(h:m)
Score ([min, mean, max] over agents) for ep. 4: [0.00,0.01,0.1] 	T: 0:21(m:s)	Est remain: 1:25(h:m)
Score ([min, mean, max] over agents) for ep. 5: [0.00,0.02,0.1] 	T: 0:20(m:s)	Est remain: 1:24(h:m)
Score ([min, mean, max] over agents) for ep. 6: [0.00,0.02,0.2] 	T: 0:21(m:s)	Est remain: 1:24(h:m)
Score ([min, mean, max] over agents) for ep. 7: [0.00,0.03,0.1] 	T: 0:21(m:s)	Est remain: 1:26(h:m)
Score ([min, mean, max] over agents) for ep. 8: [0.00,0.01,0.1] 	T: 0:21(m:s)	Est remain: 1:23(h:m)
Score ([min, mean, max] over agents) for ep. 9: [0.00,0.00,0.1] 	T: 0:21(m:s)	Est remain: 1:24(h:m)
Score ([min, mean, max] over agents) for ep. 10: [0.00,0.01,0.1] 	T: 0:21(m:s)	Est remain: 1:26(h:m)
Score ([min, mean, max] over agents) for ep. 11: [0.00,0.00,0.0] 	T: 0:21(m:s)	Est remain: 1:23(h:m)
Score ([min, mean, max] over agents) for ep. 12: [0.00,0.04,0.2] 	T: 0:23(m:s)	Est remain: 1:31(h:m)
Score ([min, mean, max] over agents) for ep. 13: [0.00,0.08,0.4] 	T: 0:22(m:s)	Est remain: 1:26(h:m)
Score ([min, mean, max] over agents) for ep. 14: [0.00,0.06,0.3] 	T: 0:21(m:s)	Est remain: 1:23(h:m)
Score ([min, mean, max] over agents) for ep. 15: [0.00,0.03,0.4] 	T: 0:21(m:s)	Est remain: 1:23(h:m)
Score ([min, mean, max] over agents) for ep. 16: [0.00,0.08,1.1] 	T: 0:22(m:s)	Est remain: 1:24(h:m)
Score ([min, mean, max] over agents) for ep. 17: [0.00,0.07,0.4] 	T: 0:22(m:s)	Est remain: 1:24(h:m)
Score ([min, mean, max] over agents) for ep. 18: [0.00,0.06,0.3] 	T: 0:21(m:s)	Est remain: 1:23(h:m)
Score ([min, mean, max] over agents) for ep. 19: [0.00,0.09,0.4] 	T: 0:22(m:s)	Est remain: 1:25(h:m)
Score ([min, mean, max] over agents) for ep. 20: [0.00,0.08,0.4] 	T: 0:22(m:s)	Est remain: 1:24(h:m)
Score ([min, mean, max] over agents) for ep. 21: [0.00,0.05,0.3] 	T: 0:23(m:s)	Est remain: 1:27(h:m)
Score ([min, mean, max] over agents) for ep. 22: [0.00,0.08,0.9] 	T: 0:23(m:s)	Est remain: 1:28(h:m)
Score ([min, mean, max] over agents) for ep. 23: [0.00,0.02,0.2] 	T: 0:22(m:s)	Est remain: 1:25(h:m)
Score ([min, mean, max] over agents) for ep. 24: [0.00,0.02,0.2] 	T: 0:22(m:s)	Est remain: 1:23(h:m)
Score ([min, mean, max] over agents) for ep. 25: [0.00,0.02,0.2] 	T: 0:23(m:s)	Est remain: 1:27(h:m)
Score ([min, mean, max] over agents) for ep. 26: [0.00,0.03,0.2] 	T: 0:22(m:s)	Est remain: 1:23(h:m)
Score ([min, mean, max] over agents) for ep. 27: [0.00,0.04,0.2] 	T: 0:23(m:s)	Est remain: 1:24(h:m)
Score ([min, mean, max] over agents) for ep. 28: [0.00,0.02,0.2] 	T: 0:23(m:s)	Est remain: 1:26(h:m)
Score ([min, mean, max] over agents) for ep. 29: [0.00,0.04,0.2] 	T: 0:23(m:s)	Est remain: 1:23(h:m)