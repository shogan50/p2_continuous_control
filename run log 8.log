time <attribute 'day' of 'datetime.date' objects> <attribute 'hour' of 'datetime.datetime' objects>:0001-01-01 00:00:00main file: DDPG_main.pyplatform = Windows
episodes = 250
buffer size = 1000000
gamma = 0.99
tau = 0.001
learning rate actor = 0.001
learning rate critic = 0.001
device = cuda:0
learn every = 20
learn repeat x times = 10
(noise) sigma = 0.2
fc1,fc2 units = 400,300
 prior attempts used 128 fc1 and fc2 units
pre-fill the buffer with experiences from random actions
Score ([min, mean, max] over agents) for ep. 0: [0.00,0.01,0.1] 	T: 0:21(m:s)	Est remain: 1:28(h:m)
Score ([min, mean, max] over agents) for ep. 1: [0.00,0.01,0.1] 	T: 0:20(m:s)	Est remain: 1:23(h:m)
Score ([min, mean, max] over agents) for ep. 2: [0.00,0.00,0.0] 	T: 0:21(m:s)	Est remain: 1:26(h:m)
Score ([min, mean, max] over agents) for ep. 3: [0.00,0.02,0.1] 	T: 0:20(m:s)	Est remain: 1:24(h:m)
Score ([min, mean, max] over agents) for ep. 4: [0.00,0.00,0.0] 	T: 0:20(m:s)	Est remain: 1:22(h:m)
Score ([min, mean, max] over agents) for ep. 5: [0.00,0.00,0.0] 	T: 0:22(m:s)	Est remain: 1:30(h:m)
Score ([min, mean, max] over agents) for ep. 6: [0.00,0.01,0.1] 	T: 0:21(m:s)	Est remain: 1:24(h:m)
Score ([min, mean, max] over agents) for ep. 7: [0.00,0.00,0.0] 	T: 0:20(m:s)	Est remain: 1:22(h:m)
Score ([min, mean, max] over agents) for ep. 8: [0.00,0.01,0.1] 	T: 0:21(m:s)	Est remain: 1:24(h:m)
Score ([min, mean, max] over agents) for ep. 9: [0.00,0.00,0.0] 	T: 0:21(m:s)	Est remain: 1:22(h:m)
Score ([min, mean, max] over agents) for ep. 10: [0.00,0.03,0.2] 	T: 0:22(m:s)	Est remain: 1:29(h:m)
Score ([min, mean, max] over agents) for ep. 11: [0.00,0.01,0.1] 	T: 0:21(m:s)	Est remain: 1:23(h:m)
Score ([min, mean, max] over agents) for ep. 12: [0.00,0.02,0.1] 	T: 0:22(m:s)	Est remain: 1:27(h:m)
Score ([min, mean, max] over agents) for ep. 13: [0.00,0.03,0.2] 	T: 0:21(m:s)	Est remain: 1:23(h:m)
Score ([min, mean, max] over agents) for ep. 14: [0.00,0.03,0.1] 	T: 0:22(m:s)	Est remain: 1:27(h:m)